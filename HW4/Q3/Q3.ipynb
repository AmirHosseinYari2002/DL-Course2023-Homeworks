{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import required packages\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import AdamW\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence"
      ],
      "metadata": {
        "id": "z38IOOexqYSe"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N4AA244MpmTT"
      },
      "outputs": [],
      "source": [
        "# Read the input text file\n",
        "with open('ferdousi.txt', 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "# Combine every two lines into one verse\n",
        "verses = [''.join(lines[i:i+2]) for i in range(0, len(lines), 2)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"HooshvareLab/gpt2-fa\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# Set the padding token to be the same as the eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Set up loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "8KazWRgD2LWa"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoetryDataset(Dataset):\n",
        "    def __init__(self, verses):\n",
        "        self.verses = verses\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.verses)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokenized = tokenizer(self.verses[idx], return_tensors=\"pt\", truncation=True, padding=True)\n",
        "        return {'input_ids': tokenized['input_ids'].squeeze(), 'attention_mask': tokenized['attention_mask'].squeeze()}\n",
        "\n",
        "def tokenize_poetry(poetry_text):\n",
        "    return tokenizer(poetry_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "\n",
        "poetry_dataset = PoetryDataset(verses)\n",
        "train_dataset, test_dataset = train_test_split(poetry_dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=lambda x: {'input_ids': pad_sequence([item['input_ids'] for item in x], batch_first=True), 'attention_mask': pad_sequence([item['attention_mask'] for item in x], batch_first=True)})\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=lambda x: {'input_ids': pad_sequence([item['input_ids'] for item in x], batch_first=True), 'attention_mask': pad_sequence([item['attention_mask'] for item in x], batch_first=True)})"
      ],
      "metadata": {
        "id": "kI2H6kEVvYJR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}\"):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49Ti4CLDw8x3",
        "outputId": "ab63397f-5713-49c1-bf29-fc2d9c95f261"
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 1396/1396 [04:30<00:00,  5.17it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Average Loss: 3.530107312011172\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 1396/1396 [04:29<00:00,  5.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2, Average Loss: 2.777094612148908\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 1396/1396 [04:29<00:00,  5.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3, Average Loss: 2.4227306385094933\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 1396/1396 [04:29<00:00,  5.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4, Average Loss: 2.1504938264289355\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 1396/1396 [04:29<00:00,  5.19it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Average Loss: 1.9087167855320142\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 1396/1396 [04:29<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 1.685677365023632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 1396/1396 [04:29<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 1.4830419136835715\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 1396/1396 [04:29<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Average Loss: 1.3057179375842511\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 1396/1396 [04:28<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Average Loss: 1.1446073001896413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 1396/1396 [04:28<00:00,  5.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Average Loss: 1.0071994590554334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on test set\n",
        "model.eval()\n",
        "total_bleu_score = 0\n",
        "total_loss = 0\n",
        "num_batches = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_dataloader):\n",
        "        input_ids = batch['input_ids'].squeeze(dim=1).to(device)\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Generate with attention_mask\n",
        "        outputs = model.generate(\n",
        "            input_ids,\n",
        "            max_length=25,\n",
        "            num_beams=5,\n",
        "            no_repeat_ngram_size=2,\n",
        "            top_k=50,\n",
        "            attention_mask=torch.ones_like(input_ids)\n",
        "        )\n",
        "\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        reference_text = tokenizer.decode(labels[0], skip_special_tokens=True)\n",
        "\n",
        "        bleu_score = corpus_bleu([[reference_text.split()]], [generated_text.split()])\n",
        "        total_bleu_score += bleu_score\n",
        "        num_batches += 1\n",
        "\n",
        "average_bleu_score = total_bleu_score / num_batches\n",
        "\n",
        "print(f\"\\nAverage BLEU Score on Test Set: {average_bleu_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCJaXSBZzKoX",
        "outputId": "23848db2-7a7a-4245-f82c-3ba673a8b819"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 156/156 [01:21<00:00,  1.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average BLEU Score on Test Set: 0.8645614696105637\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for generating poetry verses\n",
        "def generate_poetry(input_text, max_length=100):\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate with attention_mask\n",
        "    output_sequence = model.generate(\n",
        "        input_ids,\n",
        "        max_length=20,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        top_k=50,\n",
        "        attention_mask=torch.ones_like(input_ids)\n",
        "    )\n",
        "\n",
        "    # Decode and print the generated sequence\n",
        "    generated_verse = tokenizer.decode(output_sequence[0], skip_special_tokens=True)\n",
        "    print(f\"Generated Verse:\\n {generated_verse}\")\n",
        "\n",
        "# Example usage\n",
        "input_sentence = \"تو نیکی می کن\"\n",
        "generate_poetry(input_sentence)\n",
        "\n",
        "# Example usage\n",
        "input_sentence = \"سعدیا مرد نکونام\"\n",
        "generate_poetry(input_sentence)\n",
        "\n",
        "# Example usage\n",
        "input_sentence = \"سلام من به تو\"\n",
        "generate_poetry(input_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeqIQ-xNEMiQ",
        "outputId": "4376ebff-29eb-4050-b9ad-b5fbaacbc038"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Verse:\n",
            " تو نیکی می کن و رامشی کن بلند\n",
            "دلم پر ز تیمار شد چون گزند\n",
            "\n",
            "Generated Verse:\n",
            " سعدیا مرد نکونام بدست\n",
            "نیامدش خراد بر زین ببست\n",
            "\n",
            "\n",
            "Generated Verse:\n",
            " سلام من به تو یک امروز رواست\n",
            "که یک روز تو دیگر آید به مرو\n",
            "\n"
          ]
        }
      ]
    }
  ]
}