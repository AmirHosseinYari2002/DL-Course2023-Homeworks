{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import Required Packages\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from pycocotools.coco import COCO\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import fiftyone.zoo as foz\n",
        "from torchvision.io.image import read_image\n",
        "from torchvision.ops import deform_conv2d\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim import Adam\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms.functional import to_pil_image, to_grayscale, to_tensor\n",
        "import torchvision.ops\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "lKaNNaoPLTBr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeformableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):\n",
        "        super(DeformableConv2d, self).__init__()\n",
        "        self.padding = padding\n",
        "\n",
        "        # Offset convolution layer to learn the offsets of the sampling locations\n",
        "        self.offset_conv = nn.Conv2d(in_channels, 2 * kernel_size * kernel_size, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=True)\n",
        "\n",
        "        # Initialize weights and bias of offset convolution to 0\n",
        "        nn.init.constant_(self.offset_conv.weight, 0.)\n",
        "        nn.init.constant_(self.offset_conv.bias, 0.)\n",
        "\n",
        "        # Modulator convolution layer to learn the modulation scalars of the sampling locations\n",
        "        self.modulator_conv = nn.Conv2d(in_channels, 1 * kernel_size * kernel_size, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=True)\n",
        "\n",
        "        # Initialize weights and bias of modulator convolution to 0\n",
        "        nn.init.constant_(self.modulator_conv.weight, 0.)\n",
        "        nn.init.constant_(self.modulator_conv.bias, 0.)\n",
        "\n",
        "        # Regular convolution layer\n",
        "        self.regular_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=self.padding, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[2:]  # Height and width of the input\n",
        "        max_offset = max(h, w)/4.\n",
        "\n",
        "        # Compute offset and clamp it within the range [-max_offset, max_offset]\n",
        "        offset = self.offset_conv(x).clamp(-max_offset, max_offset)\n",
        "\n",
        "        # Compute modulator and apply sigmoid function scaled by 2\n",
        "        modulator = 2. * torch.sigmoid(self.modulator_conv(x))\n",
        "\n",
        "        # Apply deformable convolution\n",
        "        x = torchvision.ops.deform_conv2d(input=x, offset=offset, weight=self.regular_conv.weight, bias=self.regular_conv.bias, padding=self.padding, mask=modulator)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "kwl72sm3Li-X"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class MyCNN(nn.Module):\n",
        "    def __init__(self, classes):\n",
        "        super(MyCNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer with 3 input channels and 32 output channels\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        # Second convolutional layer with 32 input channels and 32 output channels\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        # Third convolutional layer with 32 input channels and 32 output channels\n",
        "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        # Fourth convolutional layer, a deformable convolutional layer\n",
        "        self.conv4 = DeformableConv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        # Fifth convolutional layer, a deformable convolutional layer\n",
        "        self.conv5 = DeformableConv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        # Max pooling layer with a kernel size of 2\n",
        "        self.pool = nn.MaxPool2d(2)\n",
        "        # Global average pooling layer to reduce each channel to a single value\n",
        "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        # Fully connected layer with an output size equal to the number of classes\n",
        "        self.fc = nn.Linear(32, len(classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "          # Apply the first convolutional layer followed by ReLU activation function\n",
        "          x = torch.relu(self.conv1(x))\n",
        "          # Apply max pooling\n",
        "          x = self.pool(x) # [112, 112]\n",
        "          # Apply the second convolutional layer followed by ReLU activation function\n",
        "          x = torch.relu(self.conv2(x))\n",
        "          # Apply max pooling\n",
        "          x = self.pool(x) # [56, 56]\n",
        "          # Apply the third, fourth, and fifth convolutional layers followed by ReLU activation function\n",
        "          x = torch.relu(self.conv3(x))\n",
        "          x = torch.relu(self.conv4(x))\n",
        "          x = torch.relu(self.conv5(x))\n",
        "          # Apply global average pooling\n",
        "          x = self.gap(x)\n",
        "          # Flatten the tensor from the second dimension\n",
        "          x = x.flatten(start_dim=1)\n",
        "          # Apply the fully connected layer\n",
        "          x = self.fc(x)\n",
        "          return x"
      ],
      "metadata": {
        "id": "pWZC0XNQUrhH"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class COCO(Dataset):\n",
        "    def __init__(self, datasets, classes, transforms=None):\n",
        "        # Initialize the dataset, classes, and transforms\n",
        "        self.dataset = datasets\n",
        "        self.classes = classes\n",
        "        self.transform = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the length of the dataset\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get a sample from the dataset\n",
        "        sample = self.dataset[idx]\n",
        "        # Open the image file associated with the sample and convert it to RGB\n",
        "        image = Image.open(sample.filepath).convert('RGB')\n",
        "\n",
        "        # Initialize the label to a zero vector\n",
        "        label = np.zeros(len(self.classes), dtype=np.float32)\n",
        "\n",
        "        # Assign 1 to the positions of the classes present in the image\n",
        "        for detection in sample.ground_truth.detections:\n",
        "            if detection.label in classes:\n",
        "                label[classes.index(detection.label)] = 1.0\n",
        "        # Apply the transforms to the image\n",
        "        image = self.transform(image)\n",
        "\n",
        "        # Convert the label to a PyTorch tensor\n",
        "        label = torch.tensor(label, dtype=torch.float64)\n",
        "\n",
        "        # Return the transformed image and the label\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "KYMCvmLJ4AEt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classes for which we want to load the data\n",
        "classes = ['bicycle', 'car', 'cat', 'chair', 'cow', 'dog', 'horse', 'person', 'sheep']\n",
        "\n",
        "# Load the training dataset from the COCO 2017 dataset in the FiftyOne Dataset Zoo\n",
        "# The data includes segmentations labels\n",
        "dataset_train = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"train\",\n",
        "    label_types=[\"segmentations\"],\n",
        "    classes=classes,\n",
        "    max_samples=20000,\n",
        ")\n",
        "# Convert the dataset to a list for further processing\n",
        "dataset_train = list(dataset_train)\n",
        "\n",
        "# Similarly, load the validation dataset from the COCO 2017 dataset in the FiftyOne Dataset Zoo\n",
        "# The data includes segmentations labels\n",
        "dataset_test = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    label_types=[\"segmentations\"],\n",
        "    classes=classes,\n",
        "    max_samples=2000,\n",
        ")\n",
        "# Convert the dataset to a list for further processing\n",
        "dataset_test = list(dataset_test)"
      ],
      "metadata": {
        "id": "GREnj_vNMiDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7edb2a91-a9b4-4002-f3f4-6b839cc1757d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/coco-2017/train' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_train2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'train' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'train' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-train-20000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-train-20000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'validation' to '/root/fiftyone/coco-2017/validation' if necessary\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Found annotations at '/root/fiftyone/coco-2017/raw/instances_val2017.json'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.utils.coco:Sufficient images already downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Existing download of split 'validation' is sufficient\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading existing dataset 'coco-2017-validation-2000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading existing dataset 'coco-2017-validation-2000'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if CUDA is available and set the device to GPU if it is, else use CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the transformations to be applied on the images\n",
        "# Resize the images to 224x224, convert them to tensors, and normalize them\n",
        "transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "# Create a COCO dataset for the training data\n",
        "# Apply the defined transformations on the dataset\n",
        "trainset = COCO(datasets=dataset_train, classes=classes, transforms=transforms)\n",
        "\n",
        "# Create a DataLoader for the training dataset\n",
        "# Set the batch size to 128 and shuffle the data\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "\n",
        "# Similarly, create a COCO dataset for the test data and apply the transformations\n",
        "testset = COCO(datasets=dataset_test, classes=classes, transforms=transforms)\n",
        "\n",
        "# Create a DataLoader for the test dataset\n",
        "# Set the batch size to 32 and do not shuffle the data\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Jt9ovg4lP8_G"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model = MyCNN(classes).to(device)\n",
        "\n",
        "# Define the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define the learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Set the number of training epochs\n",
        "epochs = 5\n",
        "\n",
        "# Initialize lists to store accuracy and loss values for each epoch\n",
        "accuracy_values = []\n",
        "loss_values = []\n",
        "\n",
        "# Start the training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    running_loss = 0.0\n",
        "    overall_accuracy = 0\n",
        "    accuracy_per_label = torch.zeros(len(classes), device=device)\n",
        "\n",
        "    # Iterate over the training data\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate loss\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Apply sigmoid function and threshold at 0.5 to get predictions\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_predictions = (preds == labels).float()\n",
        "\n",
        "        # Calculate the accuracy for each label\n",
        "        accuracy_per_label += correct_predictions.sum(0)/(len(labels))\n",
        "\n",
        "        # Calculate the overall accuracy\n",
        "        overall_accuracy += correct_predictions.sum()/(len(labels)*(len(classes)))\n",
        "\n",
        "    # Calculate the average accuracy for each label\n",
        "    accuracy_per_label /= len(trainloader)\n",
        "\n",
        "    # Calculate the average loss\n",
        "    running_loss /= len(trainloader)\n",
        "\n",
        "    # Calculate the overall average accuracy\n",
        "    overall_accuracy /= len(trainloader)\n",
        "\n",
        "    # Store accuracy and loss values\n",
        "    accuracy_values.append(overall_accuracy.item())\n",
        "    loss_values.append(running_loss)\n",
        "\n",
        "    # Print training statistics after each epoch\n",
        "    print(f'Epoch {epoch+1}/{epochs}:')\n",
        "    print(f'Training Loss: {running_loss:.4f}')\n",
        "    print(f'Total Accuracy: {overall_accuracy*100:.2f}%')\n",
        "    print('Per Class Accuracy:', (accuracy_per_label*100).tolist())\n",
        "\n",
        "# Plot accuracy per epoch\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, epochs+1), accuracy_values, label='Accuracy')\n",
        "plt.plot(range(1, epochs+1), loss_values, label='loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LDkMnDxoVG3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 848
        },
        "outputId": "caca599f-a932-4773-c369-e37849029d7e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5:\n",
            "Training Loss: 0.2879\n",
            "Total Accuracy: 90.02%\n",
            "Per Class Accuracy: [95.41699981689453, 81.74761199951172, 94.87957763671875, 82.82245635986328, 94.62579345703125, 92.62042236328125, 91.4311294555664, 78.39370727539062, 98.25338745117188]\n",
            "Epoch 2/5:\n",
            "Training Loss: 0.2514\n",
            "Total Accuracy: 91.77%\n",
            "Per Class Accuracy: [96.07882690429688, 84.69347381591797, 94.81986999511719, 84.19088745117188, 97.53185272216797, 94.62579345703125, 96.29776763916016, 79.46357727050781, 98.23845672607422]\n",
            "Epoch 3/5:\n",
            "Training Loss: 0.2474\n",
            "Total Accuracy: 91.76%\n",
            "Per Class Accuracy: [96.0340347290039, 84.57404327392578, 94.8945083618164, 84.22074127197266, 97.53185272216797, 94.58101654052734, 96.31269836425781, 79.47850036621094, 98.25338745117188]\n",
            "Epoch 4/5:\n",
            "Training Loss: 0.2450\n",
            "Total Accuracy: 91.76%\n",
            "Per Class Accuracy: [96.0340347290039, 84.58399963378906, 94.87957763671875, 84.17595672607422, 97.53185272216797, 94.56608581542969, 96.29776763916016, 79.48348236083984, 98.25338745117188]\n",
            "Epoch 5/5:\n",
            "Training Loss: 0.2410\n",
            "Total Accuracy: 91.75%\n",
            "Per Class Accuracy: [96.0340347290039, 84.53424072265625, 94.8646469116211, 84.14610290527344, 97.50199127197266, 94.62579345703125, 96.31269836425781, 79.44864654541016, 98.23845672607422]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the model\n",
        "with torch.no_grad():\n",
        "\n",
        "    overall_accuracy = 0\n",
        "    accuracy_per_label = torch.zeros(len(classes), device=device)\n",
        "\n",
        "    # Iterate over the test data\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Apply sigmoid function and threshold at 0.5 to get predictions\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "\n",
        "        # Calculate the correct predictions\n",
        "        correct_predictions = (preds == labels).float()\n",
        "\n",
        "        # Calculate the accuracy for each label\n",
        "        accuracy_per_label += correct_predictions.sum(0)/(len(labels))\n",
        "\n",
        "        # Calculate the overall accuracy\n",
        "        overall_accuracy += correct_predictions.sum()/(len(labels)*(len(classes)))\n",
        "\n",
        "    # Calculate the average accuracy for each label\n",
        "    accuracy_per_label /= len(testloader)\n",
        "\n",
        "    # Calculate the overall average accuracy\n",
        "    overall_accuracy /= len(testloader)\n",
        "\n",
        "    # Print testing statistics\n",
        "    print(f'Total Accuracy: {overall_accuracy*100:.2f}%')\n",
        "    print('Per Class Accuracy:', (accuracy_per_label*100).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaseLX9hVGxv",
        "outputId": "37b536de-3dab-4869-dba8-ad3ee6f6087f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Accuracy: 91.34%\n",
            "Per Class Accuracy: [95.23809814453125, 84.52381134033203, 94.39484405517578, 82.93651580810547, 97.37103271484375, 94.74207305908203, 96.08135223388672, 78.5714340209961, 98.16468811035156]\n"
          ]
        }
      ]
    }
  ]
}